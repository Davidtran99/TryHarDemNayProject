# S·ª≠a Chatbot ƒë·ªÉ H·ªó tr·ª£ General Conversation

## V·∫•n ƒë·ªÅ

Chatbot kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c nh∆∞ m·ªôt chatbot AI th√¥ng th∆∞·ªùng v√¨:
1. **Ch·ªâ g·ªçi LLM khi c√≥ documents** ‚Üí Kh√¥ng th·ªÉ tr·∫£ l·ªùi general queries
2. **Tr·∫£ v·ªÅ error message ngay khi kh√¥ng c√≥ documents** ‚Üí Kh√¥ng cho LLM c∆° h·ªôi tr·∫£ l·ªùi

## Gi·∫£i ph√°p ƒë√£ √°p d·ª•ng

### 1. S·ª≠a `rag.py` - Cho ph√©p LLM tr·∫£ l·ªùi ngay c·∫£ khi kh√¥ng c√≥ documents

**File:** `backend/hue_portal/core/rag.py`

**Thay ƒë·ªïi:**
- Tr∆∞·ªõc: Tr·∫£ v·ªÅ error message ngay khi kh√¥ng c√≥ documents
- Sau: G·ªçi LLM ngay c·∫£ khi kh√¥ng c√≥ documents (general conversation mode)

```python
# Tr∆∞·ªõc:
if not documents:
    return error_message  # ‚Üê Kh√¥ng g·ªçi LLM

# Sau:
# G·ªçi LLM tr∆∞·ªõc (ngay c·∫£ khi kh√¥ng c√≥ documents)
if use_llm:
    llm_answer = llm.generate_answer(query, context=context, documents=documents if documents else [])
    if llm_answer:
        return llm_answer

# Ch·ªâ tr·∫£ v·ªÅ error n·∫øu kh√¥ng c√≥ LLM v√† kh√¥ng c√≥ documents
if not documents:
    return error_message
```

### 2. S·ª≠a `llm_integration.py` - Prompt cho general conversation

**File:** `backend/hue_portal/chatbot/llm_integration.py`

**Thay ƒë·ªïi:**
- N·∫øu c√≥ documents ‚Üí Y√™u c·∫ßu tr·∫£ l·ªùi d·ª±a tr√™n documents (strict mode)
- N·∫øu kh√¥ng c√≥ documents ‚Üí Cho ph√©p general conversation (friendly mode)

```python
if documents:
    # Strict mode: ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n documents
    prompt_parts.extend([...])
else:
    # General conversation mode
    prompt_parts.extend([
        "- Tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch t·ª± nhi√™n v√† h·ªØu √≠ch nh∆∞ m·ªôt chatbot AI th√¥ng th∆∞·ªùng",
        "- N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn ph√°p lu·∫≠t nh∆∞ng kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i r√µ",
        ...
    ])
```

### 3. S·ª≠a `rag_pipeline` - Lu√¥n g·ªçi generate_answer_template

**File:** `backend/hue_portal/core/rag.py`

**Thay ƒë·ªïi:**
- Tr∆∞·ªõc: Tr·∫£ v·ªÅ error ngay khi kh√¥ng c√≥ documents
- Sau: Lu√¥n g·ªçi `generate_answer_template` ƒë·ªÉ cho LLM c∆° h·ªôi tr·∫£ l·ªùi

```python
# Tr∆∞·ªõc:
if not documents:
    return {'answer': error_message, ...}  # ‚Üê Kh√¥ng g·ªçi LLM

# Sau:
# Lu√¥n g·ªçi generate_answer_template (s·∫Ω g·ªçi LLM n·∫øu c√≥)
answer = generate_answer_template(query, documents, content_type, context=context, use_llm=use_llm)
```

### 4. S·ª≠a `chatbot.py` - S·ª≠ d·ª•ng answer t·ª´ LLM ngay c·∫£ khi count=0

**File:** `backend/hue_portal/chatbot/chatbot.py`

**Thay ƒë·ªïi:**
- Tr∆∞·ªõc: Ch·ªâ s·ª≠ d·ª•ng RAG result n·∫øu `count > 0`
- Sau: S·ª≠ d·ª•ng answer t·ª´ LLM ngay c·∫£ khi `count = 0`

```python
# Tr∆∞·ªõc:
if rag_result["count"] > 0 and rag_result["confidence"] >= confidence:
    # S·ª≠ d·ª•ng answer

# Sau:
if rag_result.get("answer") and (rag_result["count"] > 0 or rag_result.get("answer", "").strip()):
    # S·ª≠ d·ª•ng answer (k·ªÉ c·∫£ khi count=0)
```

## K·∫øt qu·∫£

‚úÖ **LLM ƒë∆∞·ª£c g·ªçi ngay c·∫£ khi kh√¥ng c√≥ documents**
- Logs cho th·∫•y: `[RAG] Using LLM provider: api`
- Logs cho th·∫•y: `[LLM] üîó Calling API: ...`

‚ö†Ô∏è **API tr·∫£ v·ªÅ 500 error**
- C√≥ th·ªÉ do HF Spaces API ƒëang g·∫∑p l·ªói
- Ho·∫∑c prompt qu√° d√†i
- C·∫ßn ki·ªÉm tra HF Spaces logs

## C√°ch test

1. **Test v·ªõi general query:**
```bash
curl -X POST http://localhost:8000/api/chatbot/chat/ \
  -H "Content-Type: application/json" \
  -d '{"message":"m·∫•y gi·ªù r·ªìi","reset_session":false}'
```

2. **Xem logs:**
```bash
tail -f /tmp/django_general_conv.log | grep -E "\[RAG\]|\[LLM\]"
```

3. **Ki·ªÉm tra LLM c√≥ ƒë∆∞·ª£c g·ªçi:**
- T√¨m: `[RAG] Using LLM provider: api`
- T√¨m: `[LLM] üîó Calling API: ...`

## L∆∞u √Ω

- **API mode c·∫ßn HF Spaces ho·∫°t ƒë·ªông** ‚Üí N·∫øu API tr·∫£ v·ªÅ 500, c·∫ßn ki·ªÉm tra HF Spaces
- **Local mode** s·∫Ω ho·∫°t ƒë·ªông t·ªët h∆°n n·∫øu c√≥ GPU
- **General conversation** ch·ªâ ho·∫°t ƒë·ªông khi LLM available




